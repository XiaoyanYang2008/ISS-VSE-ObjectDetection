from __future__ import print_function
from __future__ import absolute_import

import math
import os
import xml.etree.ElementTree as ET
import cv2
import copy
import threading
import itertools
import random
import pprint
import sys
import time
import numpy as np
import pandas as pd
from optparse import OptionParser
import pickle

import tensorflow as tf
from tensorflow.keras.layers import Conv2D, add, Dropout
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras import Model
from tensorflow.keras import initializers, regularizers
from tensorflow.keras import backend as K

from tensorflow.keras import Sequential
from tensorflow.keras import optimizers
from tensorflow.keras.regularizers import l2

from tensorflow.keras.layers import Input, Add, Dense, Activation, Flatten, Convolution2D, MaxPooling2D, \
    BatchNormalization
from tensorflow.keras.layers import ZeroPadding2D, AveragePooling2D, TimeDistributed
from tensorflow.keras.layers import Layer, InputSpec
from tensorflow.keras.losses import categorical_crossentropy
from tensorflow.keras.optimizers import Adam, SGD, RMSprop

# from keras_frcnn.RoiPoolingConv import RoiPoolingConv
# from keras_frcnn.FixedBatchNormalization import FixedBatchNormalization

from tensorflow.python.framework import ops
from tensorflow.python.ops import variables as variables_module
from tensorflow.python.framework import sparse_tensor
from tensorflow.python.keras.utils import generic_utils

'''
code base mainly from https://github.com/kbardool/keras-frcnn
Restructure to fit into this script.
'''


class Config:

    def __init__(self):
        self.verbose = True

        self.network = 'resnet50'

        # setting for data augmentation
        self.use_horizontal_flips = False
        self.use_vertical_flips = False
        self.rot_90 = False

        # anchor box scales
        self.anchor_box_scales = [64, 128, 256, 512]

        # anchor box ratios
        self.anchor_box_ratios = [[1, 1], [1. / math.sqrt(2), 2. / math.sqrt(2)],
                                  [2. / math.sqrt(2), 1. / math.sqrt(2)]]

        # size to resize the smallest side of the image
        self.im_size = 600

        # image channel-wise mean to subtract
        self.img_channel_mean = [103.939, 116.779, 123.68]
        self.img_scaling_factor = 1.0

        # number of ROIs at once
        # self.num_rois = 4
        self.num_rois = 2

        # stride at the RPN (this depends on the network configuration)
        self.rpn_stride = 16

        self.balanced_classes = False

        # scaling the stdev
        self.std_scaling = 4.0
        self.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]

        # overlaps for RPN
        self.rpn_min_overlap = 0.3
        self.rpn_max_overlap = 0.7

        # overlaps for classifier ROIs
        self.classifier_min_overlap = 0.1
        self.classifier_max_overlap = 0.5

        # placeholder for the class mapping, automatically generated by the parser
        self.class_mapping = None

        # location of pretrained weights for the base network
        # weight files can be found at:
        # https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_th_dim_ordering_th_kernels_notop.h5
        # https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5

        self.model_path = 'model_frcnn.vgg.hdf5'


lambda_rpn_regr = 1.0
lambda_rpn_class = 1.0

lambda_cls_regr = 1.0
lambda_cls_class = 1.0

epsilon = 1e-4


def image_dim_ordering(K):
    if K.image_data_format() == 'channels_last':
        return 'tf'
    else:
        return 'th'


def is_keras_tensor(x):
    if not isinstance(x, (ops.Tensor,
                          variables_module.Variable,
                          sparse_tensor.SparseTensor)):
        raise ValueError('Unexpectedly found an instance of type `' + str(type(x)) +
                         '`. Expected a symbolic tensor instance.')

    return hasattr(x, '_keras_history')


def augment(img_data, config, augment=True):
    assert 'filepath' in img_data
    assert 'bboxes' in img_data
    assert 'width' in img_data
    assert 'height' in img_data

    img_data_aug = copy.deepcopy(img_data)

    img = cv2.imread(img_data_aug['filepath'])

    if augment:
        rows, cols = img.shape[:2]

        if config.use_horizontal_flips and np.random.randint(0, 2) == 0:
            img = cv2.flip(img, 1)
            for bbox in img_data_aug['bboxes']:
                x1 = bbox['x1']
                x2 = bbox['x2']
                bbox['x2'] = cols - x1
                bbox['x1'] = cols - x2

        if config.use_vertical_flips and np.random.randint(0, 2) == 0:
            img = cv2.flip(img, 0)
            for bbox in img_data_aug['bboxes']:
                y1 = bbox['y1']
                y2 = bbox['y2']
                bbox['y2'] = rows - y1
                bbox['y1'] = rows - y2

        if config.rot_90:
            angle = np.random.choice([0, 90, 180, 270], 1)[0]
            if angle == 270:
                img = np.transpose(img, (1, 0, 2))
                img = cv2.flip(img, 0)
            elif angle == 180:
                img = cv2.flip(img, -1)
            elif angle == 90:
                img = np.transpose(img, (1, 0, 2))
                img = cv2.flip(img, 1)
            elif angle == 0:
                pass

            for bbox in img_data_aug['bboxes']:
                x1 = bbox['x1']
                x2 = bbox['x2']
                y1 = bbox['y1']
                y2 = bbox['y2']
                if angle == 270:
                    bbox['x1'] = y1
                    bbox['x2'] = y2
                    bbox['y1'] = cols - x2
                    bbox['y2'] = cols - x1
                elif angle == 180:
                    bbox['x2'] = cols - x1
                    bbox['x1'] = cols - x2
                    bbox['y2'] = rows - y1
                    bbox['y1'] = rows - y2
                elif angle == 90:
                    bbox['x1'] = rows - y2
                    bbox['x2'] = rows - y1
                    bbox['y1'] = x1
                    bbox['y2'] = x2
                elif angle == 0:
                    pass

    img_data_aug['width'] = img.shape[1]
    img_data_aug['height'] = img.shape[0]
    return img_data_aug, img


class data_generators:
    @staticmethod
    def union(au, bu, area_intersection):
        area_a = (au[2] - au[0]) * (au[3] - au[1])
        area_b = (bu[2] - bu[0]) * (bu[3] - bu[1])
        area_union = area_a + area_b - area_intersection
        return area_union

    @staticmethod
    def intersection(ai, bi):
        x = max(ai[0], bi[0])
        y = max(ai[1], bi[1])
        w = min(ai[2], bi[2]) - x
        h = min(ai[3], bi[3]) - y
        if w < 0 or h < 0:
            return 0
        return w * h

    @staticmethod
    def iou(a, b):
        # a and b should be (x1,y1,x2,y2)

        if a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:
            return 0.0

        area_i = data_generators.intersection(a, b)
        area_u = data_generators.union(a, b, area_i)

        return float(area_i) / float(area_u + 1e-6)

    @staticmethod
    def get_new_img_size(width, height, img_min_side=600):
        if width <= height:
            f = float(img_min_side) / width
            resized_height = int(f * height)
            resized_width = img_min_side
        else:
            f = float(img_min_side) / height
            resized_width = int(f * width)
            resized_height = img_min_side

        return resized_width, resized_height

    class SampleSelector:
        def __init__(self, class_count):
            # ignore classes that have zero samples
            self.classes = [b for b in class_count.keys() if class_count[b] > 0]
            self.class_cycle = itertools.cycle(self.classes)
            self.curr_class = next(self.class_cycle)

        def skip_sample_for_balanced_class(self, img_data):

            class_in_img = False

            for bbox in img_data['bboxes']:

                cls_name = bbox['class']

                if cls_name == self.curr_class:
                    class_in_img = True
                    self.curr_class = next(self.class_cycle)
                    break

            if class_in_img:
                return False
            else:
                return True

    @staticmethod
    def calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):

        downscale = float(C.rpn_stride)
        anchor_sizes = C.anchor_box_scales
        anchor_ratios = C.anchor_box_ratios
        num_anchors = len(anchor_sizes) * len(anchor_ratios)

        # calculate the output map size based on the network architecture

        (output_width, output_height) = img_length_calc_function(resized_width, resized_height)

        n_anchratios = len(anchor_ratios)

        # initialise empty output objectives
        y_rpn_overlap = np.zeros((output_height, output_width, num_anchors))
        y_is_box_valid = np.zeros((output_height, output_width, num_anchors))
        y_rpn_regr = np.zeros((output_height, output_width, num_anchors * 4))

        num_bboxes = len(img_data['bboxes'])

        num_anchors_for_bbox = np.zeros(num_bboxes).astype(int)
        best_anchor_for_bbox = -1 * np.ones((num_bboxes, 4)).astype(int)
        best_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)
        best_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)
        best_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)

        # get the GT box coordinates, and resize to account for image resizing
        gta = np.zeros((num_bboxes, 4))
        for bbox_num, bbox in enumerate(img_data['bboxes']):
            # get the GT box coordinates, and resize to account for image resizing
            gta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))
            gta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))
            gta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))
            gta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))

        # rpn ground truth

        for anchor_size_idx in range(len(anchor_sizes)):
            for anchor_ratio_idx in range(n_anchratios):
                anchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]
                anchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]

                for ix in range(output_width):
                    # x-coordinates of the current anchor box
                    x1_anc = downscale * (ix + 0.5) - anchor_x / 2
                    x2_anc = downscale * (ix + 0.5) + anchor_x / 2

                    # ignore boxes that go across image boundaries
                    if x1_anc < 0 or x2_anc > resized_width:
                        continue

                    for jy in range(output_height):

                        # y-coordinates of the current anchor box
                        y1_anc = downscale * (jy + 0.5) - anchor_y / 2
                        y2_anc = downscale * (jy + 0.5) + anchor_y / 2

                        # ignore boxes that go across image boundaries
                        if y1_anc < 0 or y2_anc > resized_height:
                            continue

                        # bbox_type indicates whether an anchor should be a target
                        bbox_type = 'neg'

                        # this is the best IOU for the (x,y) coord and the current anchor
                        # note that this is different from the best IOU for a GT bbox
                        best_iou_for_loc = 0.0

                        for bbox_num in range(num_bboxes):

                            # get IOU of the current GT box and the current anchor box
                            curr_iou = data_generators.iou(
                                [gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]],
                                [x1_anc, y1_anc, x2_anc, y2_anc])
                            # calculate the regression targets if they will be needed
                            if curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:
                                cx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0
                                cy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0
                                cxa = (x1_anc + x2_anc) / 2.0
                                cya = (y1_anc + y2_anc) / 2.0

                                tx = (cx - cxa) / (x2_anc - x1_anc)
                                ty = (cy - cya) / (y2_anc - y1_anc)
                                tw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))
                                th = np.log((gta[bbox_num, 3] - gta[bbox_num, 2]) / (y2_anc - y1_anc))

                            if img_data['bboxes'][bbox_num]['class'] != 'bg':

                                # all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best
                                if curr_iou > best_iou_for_bbox[bbox_num]:
                                    best_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]
                                    best_iou_for_bbox[bbox_num] = curr_iou
                                    best_x_for_bbox[bbox_num, :] = [x1_anc, x2_anc, y1_anc, y2_anc]
                                    best_dx_for_bbox[bbox_num, :] = [tx, ty, tw, th]

                                # we set the anchor to positive if the IOU is >0.7 (it does not matter if there was another better box, it just indicates overlap)
                                if curr_iou > C.rpn_max_overlap:
                                    bbox_type = 'pos'
                                    num_anchors_for_bbox[bbox_num] += 1
                                    # we update the regression layer target if this IOU is the best for the current (x,y) and anchor position
                                    if curr_iou > best_iou_for_loc:
                                        best_iou_for_loc = curr_iou
                                        best_regr = (tx, ty, tw, th)

                                # if the IOU is >0.3 and <0.7, it is ambiguous and no included in the objective
                                if C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:
                                    # gray zone between neg and pos
                                    if bbox_type != 'pos':
                                        bbox_type = 'neutral'

                        # turn on or off outputs depending on IOUs
                        if bbox_type == 'neg':
                            y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1
                            y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0
                        elif bbox_type == 'neutral':
                            y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0
                            y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0
                        elif bbox_type == 'pos':
                            y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1
                            y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1
                            start = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)
                            y_rpn_regr[jy, ix, start:start + 4] = best_regr

        # we ensure that every bbox has at least one positive RPN region

        for idx in range(num_anchors_for_bbox.shape[0]):
            if num_anchors_for_bbox[idx] == 0:
                # no box with an IOU greater than zero ...
                if best_anchor_for_bbox[idx, 0] == -1:
                    continue
                y_is_box_valid[
                    best_anchor_for_bbox[idx, 0], best_anchor_for_bbox[idx, 1], best_anchor_for_bbox[
                        idx, 2] + n_anchratios *
                    best_anchor_for_bbox[idx, 3]] = 1
                y_rpn_overlap[
                    best_anchor_for_bbox[idx, 0], best_anchor_for_bbox[idx, 1], best_anchor_for_bbox[
                        idx, 2] + n_anchratios *
                    best_anchor_for_bbox[idx, 3]] = 1
                start = 4 * (best_anchor_for_bbox[idx, 2] + n_anchratios * best_anchor_for_bbox[idx, 3])
                y_rpn_regr[
                best_anchor_for_bbox[idx, 0], best_anchor_for_bbox[idx, 1], start:start + 4] = best_dx_for_bbox[idx, :]

        y_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))
        y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)

        y_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))
        y_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)

        y_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))
        y_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)

        pos_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 1, y_is_box_valid[0, :, :, :] == 1))
        neg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))

        num_pos = len(pos_locs[0])

        # one issue is that the RPN has many more negative than positive regions, so we turn off some of the negative
        # regions. We also limit it to 256 regions.
        num_regions = 256

        if len(pos_locs[0]) > num_regions / 2:
            val_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions / 2)
            y_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0
            num_pos = num_regions / 2

        if len(neg_locs[0]) + num_pos > num_regions:
            val_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)
            y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0

        y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)
        y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=1), y_rpn_regr], axis=1)

        return np.copy(y_rpn_cls), np.copy(y_rpn_regr)

    class threadsafe_iter:
        """Takes an iterator/generator and makes it thread-safe by
        serializing call to the `next` method of given iterator/generator.
        """

        def __init__(self, it):
            self.it = it
            self.lock = threading.Lock()

        def __iter__(self):
            return self

        def next(self):
            with self.lock:
                return next(self.it)

    @staticmethod
    def threadsafe_generator(f):
        """A decorator that takes a generator function and makes it thread-safe.
        """

        def g(*a, **kw):
            return data_generators.threadsafe_iter(f(*a, **kw))

        return g

    @staticmethod
    def get_anchor_gt(all_img_data, class_count, C: Config, img_length_calc_function, backend, mode='train'):

        # The following line is not useful with Python 3.5, it is kept for the legacy
        # all_img_data = sorted(all_img_data)

        sample_selector = data_generators.SampleSelector(class_count)

        while True:
            if mode == 'train':
                np.random.shuffle(all_img_data)

            for img_data in all_img_data:
                try:

                    if C.balanced_classes and sample_selector.skip_sample_for_balanced_class(img_data):
                        continue

                    # read in image, and optionally add augmentation

                    if mode == 'train':
                        img_data_aug, x_img = augment(img_data, C, augment=True)
                    else:
                        img_data_aug, x_img = augment(img_data, C, augment=False)

                    (width, height) = (img_data_aug['width'], img_data_aug['height'])
                    (rows, cols, _) = x_img.shape

                    assert cols == width
                    assert rows == height

                    # get image dimensions for resizing
                    (resized_width, resized_height) = data_generators.get_new_img_size(width, height, C.im_size)

                    # resize the image so that smalles side is length = 600px
                    x_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)

                    try:
                        y_rpn_cls, y_rpn_regr = data_generators.calc_rpn(C, img_data_aug, width, height, resized_width,
                                                                         resized_height,
                                                                         img_length_calc_function)
                    except:
                        continue

                    # Zero-center by mean pixel, and preprocess image

                    x_img = x_img[:, :, (2, 1, 0)]  # BGR -> RGB
                    x_img = x_img.astype(np.float32)
                    x_img[:, :, 0] -= C.img_channel_mean[0]
                    x_img[:, :, 1] -= C.img_channel_mean[1]
                    x_img[:, :, 2] -= C.img_channel_mean[2]
                    x_img /= C.img_scaling_factor

                    x_img = np.transpose(x_img, (2, 0, 1))
                    x_img = np.expand_dims(x_img, axis=0)

                    y_rpn_regr[:, y_rpn_regr.shape[1] // 2:, :, :] *= C.std_scaling

                    if backend == 'tf':
                        x_img = np.transpose(x_img, (0, 2, 3, 1))
                        y_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))
                        y_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))

                    yield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug

                except Exception as e:
                    print(e)
                    continue


class roi_helper:

    @staticmethod
    def calc_iou(R, img_data, C, class_mapping):

        bboxes = img_data['bboxes']
        (width, height) = (img_data['width'], img_data['height'])
        # get image dimensions for resizing
        (resized_width, resized_height) = data_generators.get_new_img_size(width, height, C.im_size)

        gta = np.zeros((len(bboxes), 4))

        for bbox_num, bbox in enumerate(bboxes):
            # get the GT box coordinates, and resize to account for image resizing
            gta[bbox_num, 0] = int(round(bbox['x1'] * (resized_width / float(width)) / C.rpn_stride))
            gta[bbox_num, 1] = int(round(bbox['x2'] * (resized_width / float(width)) / C.rpn_stride))
            gta[bbox_num, 2] = int(round(bbox['y1'] * (resized_height / float(height)) / C.rpn_stride))
            gta[bbox_num, 3] = int(round(bbox['y2'] * (resized_height / float(height)) / C.rpn_stride))

        x_roi = []
        y_class_num = []
        y_class_regr_coords = []
        y_class_regr_label = []
        IoUs = []  # for debugging only

        for ix in range(R.shape[0]):
            (x1, y1, x2, y2) = R[ix, :]
            x1 = int(round(x1))
            y1 = int(round(y1))
            x2 = int(round(x2))
            y2 = int(round(y2))

            best_iou = 0.0
            best_bbox = -1
            for bbox_num in range(len(bboxes)):
                curr_iou = data_generators.iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]],
                                               [x1, y1, x2, y2])
                if curr_iou > best_iou:
                    best_iou = curr_iou
                    best_bbox = bbox_num

            if best_iou < C.classifier_min_overlap:
                continue
            else:
                w = x2 - x1
                h = y2 - y1
                x_roi.append([x1, y1, w, h])
                IoUs.append(best_iou)

                if C.classifier_min_overlap <= best_iou < C.classifier_max_overlap:
                    # hard negative example
                    cls_name = 'bg'
                elif C.classifier_max_overlap <= best_iou:
                    cls_name = bboxes[best_bbox]['class']
                    cxg = (gta[best_bbox, 0] + gta[best_bbox, 1]) / 2.0
                    cyg = (gta[best_bbox, 2] + gta[best_bbox, 3]) / 2.0

                    cx = x1 + w / 2.0
                    cy = y1 + h / 2.0

                    tx = (cxg - cx) / float(w)
                    ty = (cyg - cy) / float(h)
                    tw = np.log((gta[best_bbox, 1] - gta[best_bbox, 0]) / float(w))
                    th = np.log((gta[best_bbox, 3] - gta[best_bbox, 2]) / float(h))
                else:
                    print('roi = {}'.format(best_iou))
                    raise RuntimeError

            class_num = class_mapping[cls_name]
            class_label = len(class_mapping) * [0]
            class_label[class_num] = 1
            y_class_num.append(copy.deepcopy(class_label))
            coords = [0] * 4 * (len(class_mapping) - 1)
            labels = [0] * 4 * (len(class_mapping) - 1)
            if cls_name != 'bg':
                label_pos = 4 * class_num
                sx, sy, sw, sh = C.classifier_regr_std
                coords[label_pos:4 + label_pos] = [sx * tx, sy * ty, sw * tw, sh * th]
                labels[label_pos:4 + label_pos] = [1, 1, 1, 1]
                y_class_regr_coords.append(copy.deepcopy(coords))
                y_class_regr_label.append(copy.deepcopy(labels))
            else:
                y_class_regr_coords.append(copy.deepcopy(coords))
                y_class_regr_label.append(copy.deepcopy(labels))

        if len(x_roi) == 0:
            return None, None, None, None

        X = np.array(x_roi)
        Y1 = np.array(y_class_num)
        Y2 = np.concatenate([np.array(y_class_regr_label), np.array(y_class_regr_coords)], axis=1)

        return np.expand_dims(X, axis=0), np.expand_dims(Y1, axis=0), np.expand_dims(Y2, axis=0), IoUs

    @staticmethod
    def apply_regr(x, y, w, h, tx, ty, tw, th):
        try:
            cx = x + w / 2.
            cy = y + h / 2.
            cx1 = tx * w + cx
            cy1 = ty * h + cy
            w1 = math.exp(tw) * w
            h1 = math.exp(th) * h
            x1 = cx1 - w1 / 2.
            y1 = cy1 - h1 / 2.
            x1 = int(round(x1))
            y1 = int(round(y1))
            w1 = int(round(w1))
            h1 = int(round(h1))

            return x1, y1, w1, h1

        except ValueError:
            return x, y, w, h
        except OverflowError:
            return x, y, w, h
        except Exception as e:
            print(e)
            return x, y, w, h

    @staticmethod
    def apply_regr_np(X, T):
        try:
            x = X[0, :, :]
            y = X[1, :, :]
            w = X[2, :, :]
            h = X[3, :, :]

            tx = T[0, :, :]
            ty = T[1, :, :]
            tw = T[2, :, :]
            th = T[3, :, :]

            cx = x + w / 2.
            cy = y + h / 2.
            cx1 = tx * w + cx
            cy1 = ty * h + cy

            w1 = np.exp(tw.astype(np.float64)) * w
            h1 = np.exp(th.astype(np.float64)) * h
            x1 = cx1 - w1 / 2.
            y1 = cy1 - h1 / 2.

            x1 = np.round(x1)
            y1 = np.round(y1)
            w1 = np.round(w1)
            h1 = np.round(h1)
            return np.stack([x1, y1, w1, h1])
        except Exception as e:
            print(e)
            return X

    @staticmethod
    def non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):
        # code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/
        # if there are no boxes, return an empty list
        if len(boxes) == 0:
            return []

        # grab the coordinates of the bounding boxes
        x1 = boxes[:, 0]
        y1 = boxes[:, 1]
        x2 = boxes[:, 2]
        y2 = boxes[:, 3]

        np.testing.assert_array_less(x1, x2)
        np.testing.assert_array_less(y1, y2)

        # if the bounding boxes integers, convert them to floats --
        # this is important since we'll be doing a bunch of divisions
        if boxes.dtype.kind == "i":
            boxes = boxes.astype("float")

        # initialize the list of picked indexes
        pick = []

        # calculate the areas
        area = (x2 - x1) * (y2 - y1)

        # sort the bounding boxes
        idxs = np.argsort(probs)

        # keep looping while some indexes still remain in the indexes
        # list
        while len(idxs) > 0:
            # grab the last index in the indexes list and add the
            # index value to the list of picked indexes
            last = len(idxs) - 1
            i = idxs[last]
            pick.append(i)

            # find the intersection

            xx1_int = np.maximum(x1[i], x1[idxs[:last]])
            yy1_int = np.maximum(y1[i], y1[idxs[:last]])
            xx2_int = np.minimum(x2[i], x2[idxs[:last]])
            yy2_int = np.minimum(y2[i], y2[idxs[:last]])

            ww_int = np.maximum(0, xx2_int - xx1_int)
            hh_int = np.maximum(0, yy2_int - yy1_int)

            area_int = ww_int * hh_int

            # find the union
            area_union = area[i] + area[idxs[:last]] - area_int

            # compute the ratio of overlap
            overlap = area_int / (area_union + 1e-6)

            # delete all indexes from the index list that have
            idxs = np.delete(idxs, np.concatenate(([last],
                                                   np.where(overlap > overlap_thresh)[0])))

            if len(pick) >= max_boxes:
                break

        # return only the bounding boxes that were picked using the integer data type
        boxes = boxes[pick].astype("int")
        probs = probs[pick]
        return boxes, probs

    @staticmethod
    def rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=300, overlap_thresh=0.9):

        regr_layer = regr_layer / C.std_scaling

        anchor_sizes = C.anchor_box_scales
        anchor_ratios = C.anchor_box_ratios

        assert rpn_layer.shape[0] == 1

        if dim_ordering == 'th':
            (rows, cols) = rpn_layer.shape[2:]

        elif dim_ordering == 'tf':
            (rows, cols) = rpn_layer.shape[1:3]

        curr_layer = 0
        if dim_ordering == 'tf':
            A = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))
        elif dim_ordering == 'th':
            A = np.zeros((4, rpn_layer.shape[2], rpn_layer.shape[3], rpn_layer.shape[1]))

        for anchor_size in anchor_sizes:
            for anchor_ratio in anchor_ratios:

                anchor_x = (anchor_size * anchor_ratio[0]) / C.rpn_stride
                anchor_y = (anchor_size * anchor_ratio[1]) / C.rpn_stride
                if dim_ordering == 'th':
                    regr = regr_layer[0, 4 * curr_layer:4 * curr_layer + 4, :, :]
                else:
                    regr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4]
                    regr = np.transpose(regr, (2, 0, 1))

                X, Y = np.meshgrid(np.arange(cols), np.arange(rows))

                A[0, :, :, curr_layer] = X - anchor_x / 2
                A[1, :, :, curr_layer] = Y - anchor_y / 2
                A[2, :, :, curr_layer] = anchor_x
                A[3, :, :, curr_layer] = anchor_y

                if use_regr:
                    A[:, :, :, curr_layer] = roi_helper.apply_regr_np(A[:, :, :, curr_layer], regr)

                A[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])
                A[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])
                A[2, :, :, curr_layer] += A[0, :, :, curr_layer]
                A[3, :, :, curr_layer] += A[1, :, :, curr_layer]

                A[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])
                A[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])
                A[2, :, :, curr_layer] = np.minimum(cols - 1, A[2, :, :, curr_layer])
                A[3, :, :, curr_layer] = np.minimum(rows - 1, A[3, :, :, curr_layer])

                curr_layer += 1

        all_boxes = np.reshape(A.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0))
        all_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))

        x1 = all_boxes[:, 0]
        y1 = all_boxes[:, 1]
        x2 = all_boxes[:, 2]
        y2 = all_boxes[:, 3]

        idxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))

        all_boxes = np.delete(all_boxes, idxs, 0)
        all_probs = np.delete(all_probs, idxs, 0)

        result = \
            roi_helper.non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh,
                                                max_boxes=max_boxes)[0]

        return result


class pascal_voc_parser:

    @staticmethod
    def get_data(input_path):
        all_imgs = []

        classes_count = {}

        class_mapping = {}

        visualise = False

        data_paths = [os.path.join(input_path, s) for s in ['VOC2012']]

        print('Parsing annotation files')

        for data_path in data_paths:

            annot_path = os.path.join(data_path, 'Annotations')
            imgs_path = os.path.join(data_path, 'JPEGImages')
            imgsets_path_trainval = os.path.join(data_path, 'ImageSets', 'Main', 'trainval.txt')
            imgsets_path_test = os.path.join(data_path, 'ImageSets', 'Main', 'test.txt')

            trainval_files = []
            test_files = []
            try:
                with open(imgsets_path_trainval) as f:
                    for line in f:
                        trainval_files.append(line.strip() + '.jpg')
            except Exception as e:
                print(e)

            try:
                with open(imgsets_path_test) as f:
                    for line in f:
                        test_files.append(line.strip() + '.jpg')
            except Exception as e:
                if data_path[-7:] == 'VOC2012':
                    # this is expected, most pascal voc distibutions dont have the test.txt file
                    pass
                else:
                    print(e)

            annots = [os.path.join(annot_path, s) for s in os.listdir(annot_path)]
            idx = 0
            for annot in annots:
                try:
                    idx += 1

                    et = ET.parse(annot)
                    element = et.getroot()

                    element_objs = element.findall('object')
                    element_filename = element.find('filename').text
                    element_width = int(element.find('size').find('width').text)
                    element_height = int(element.find('size').find('height').text)

                    if len(element_objs) > 0:
                        annotation_data = {'filepath': os.path.join(imgs_path, element_filename),
                                           'width': element_width,
                                           'height': element_height, 'bboxes': []}

                        if element_filename in trainval_files:
                            annotation_data['imageset'] = 'trainval'
                        elif element_filename in test_files:
                            annotation_data['imageset'] = 'test'
                        else:
                            annotation_data['imageset'] = 'trainval'

                    for element_obj in element_objs:
                        class_name = element_obj.find('name').text
                        if class_name not in classes_count:
                            classes_count[class_name] = 1
                        else:
                            classes_count[class_name] += 1

                        if class_name not in class_mapping:
                            class_mapping[class_name] = len(class_mapping)

                        obj_bbox = element_obj.find('bndbox')
                        x1 = int(round(float(obj_bbox.find('xmin').text)))
                        y1 = int(round(float(obj_bbox.find('ymin').text)))
                        x2 = int(round(float(obj_bbox.find('xmax').text)))
                        y2 = int(round(float(obj_bbox.find('ymax').text)))
                        difficulty = int(element_obj.find('difficult').text) == 1
                        annotation_data['bboxes'].append(
                            {'class': class_name, 'x1': x1, 'x2': x2, 'y1': y1, 'y2': y2, 'difficult': difficulty})
                    all_imgs.append(annotation_data)

                    if visualise:
                        img = cv2.imread(annotation_data['filepath'])
                        for bbox in annotation_data['bboxes']:
                            cv2.rectangle(img, (bbox['x1'], bbox['y1']), (bbox[
                                                                              'x2'], bbox['y2']), (0, 0, 255))
                        cv2.imshow('img', img)
                        cv2.waitKey(0)

                except Exception as e:
                    print(e)
                    continue
        return all_imgs, classes_count, class_mapping


class lossers:

    @staticmethod
    def rpn_loss_regr(num_anchors):
        def rpn_loss_regr_fixed_num(y_true, y_pred):
            if image_dim_ordering(K) == 'th':
                x = y_true[:, 4 * num_anchors:, :, :] - y_pred
                x_abs = K.abs(x)
                x_bool = K.less_equal(x_abs, 1.0)
                return lambda_rpn_regr * K.sum(
                    y_true[:, :4 * num_anchors, :, :] * (
                            x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(
                    epsilon + y_true[:, :4 * num_anchors, :, :])
            else:
                x = y_true[:, :, :, 4 * num_anchors:] - y_pred
                x_abs = K.abs(x)
                x_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)

                return lambda_rpn_regr * K.sum(
                    y_true[:, :, :, :4 * num_anchors] * (
                            x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(
                    epsilon + y_true[:, :, :, :4 * num_anchors])

        return rpn_loss_regr_fixed_num

    @staticmethod
    def rpn_loss_cls(num_anchors):
        def rpn_loss_cls_fixed_num(y_true, y_pred):
            if image_dim_ordering(K) == 'tf':
                return lambda_rpn_class * K.sum(
                    y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :],
                                                                          y_true[:, :, :,
                                                                          num_anchors:])) / K.sum(
                    epsilon + y_true[:, :, :, :num_anchors])
            else:
                return lambda_rpn_class * K.sum(
                    y_true[:, :num_anchors, :, :] * K.binary_crossentropy(y_pred[:, :, :, :],
                                                                          y_true[:,
                                                                          num_anchors:, :,
                                                                          :])) / K.sum(
                    epsilon + y_true[:, :num_anchors, :, :])

        return rpn_loss_cls_fixed_num

    @staticmethod
    def class_loss_regr(num_classes):
        def class_loss_regr_fixed_num(y_true, y_pred):
            x = y_true[:, :, 4 * num_classes:] - y_pred
            x_abs = K.abs(x)
            x_bool = K.cast(K.less_equal(x_abs, 1.0), 'float32')
            return lambda_cls_regr * K.sum(
                y_true[:, :, :4 * num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(
                epsilon + y_true[:, :, :4 * num_classes])

        return class_loss_regr_fixed_num

    @staticmethod
    def class_loss_cls(y_true, y_pred):
        return lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))
        ## ValueError: Index out of range using input dim 2; input has only 2 dims for 'loss_1/dense_class_21_loss/strided_slice' (op: 'StridedSlice') with input shapes: [?,?], [3], [3], [3] and with computed input tensors: input[3] = <1 1 1>.


class FixedBatchNormalization(Layer):

    def __init__(self, epsilon=1e-3, axis=-1,
                 weights=None, beta_init='zero', gamma_init='one',
                 gamma_regularizer=None, beta_regularizer=None, **kwargs):

        self.supports_masking = True
        self.beta_init = initializers.get(beta_init)
        self.gamma_init = initializers.get(gamma_init)
        self.epsilon = epsilon
        self.axis = axis
        self.gamma_regularizer = regularizers.get(gamma_regularizer)
        self.beta_regularizer = regularizers.get(beta_regularizer)
        self.initial_weights = weights
        super(FixedBatchNormalization, self).__init__(**kwargs)

    def build(self, input_shape):
        self.input_spec = [InputSpec(shape=input_shape)]
        shape = (input_shape[self.axis],)

        self.gamma = self.add_weight(shape=shape,
                                     initializer=self.gamma_init,
                                     regularizer=self.gamma_regularizer,
                                     name='{}_gamma'.format(self.name),
                                     trainable=False)
        self.beta = self.add_weight(shape=shape,
                                    initializer=self.beta_init,
                                    regularizer=self.beta_regularizer,
                                    name='{}_beta'.format(self.name),
                                    trainable=False)
        self.running_mean = self.add_weight(shape=shape, initializer='zero',
                                            name='{}_running_mean'.format(self.name),
                                            trainable=False)
        self.running_std = self.add_weight(shape=shape, initializer='one',
                                           name='{}_running_std'.format(self.name),
                                           trainable=False)

        if self.initial_weights is not None:
            self.set_weights(self.initial_weights)
            del self.initial_weights

        self.built = True

    def call(self, x, mask=None):

        assert self.built, 'Layer must be built before being called'
        input_shape = K.int_shape(x)

        reduction_axes = list(range(len(input_shape)))
        del reduction_axes[self.axis]
        broadcast_shape = [1] * len(input_shape)
        broadcast_shape[self.axis] = input_shape[self.axis]

        if sorted(reduction_axes) == range(K.ndim(x))[:-1]:
            x_normed = K.batch_normalization(
                x, self.running_mean, self.running_std,
                self.beta, self.gamma,
                epsilon=self.epsilon)
        else:
            # need broadcasting
            broadcast_running_mean = K.reshape(self.running_mean, broadcast_shape)
            broadcast_running_std = K.reshape(self.running_std, broadcast_shape)
            broadcast_beta = K.reshape(self.beta, broadcast_shape)
            broadcast_gamma = K.reshape(self.gamma, broadcast_shape)
            x_normed = K.batch_normalization(
                x, broadcast_running_mean, broadcast_running_std,
                broadcast_beta, broadcast_gamma,
                epsilon=self.epsilon)

        return x_normed

    def get_config(self):
        config = {'epsilon': self.epsilon,
                  'axis': self.axis,
                  'gamma_regularizer': self.gamma_regularizer.get_config() if self.gamma_regularizer else None,
                  'beta_regularizer': self.beta_regularizer.get_config() if self.beta_regularizer else None}
        base_config = super(FixedBatchNormalization, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


class RoiPoolingConv(Layer):
    '''ROI pooling layer for 2D inputs.
    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,
    K. He, X. Zhang, S. Ren, J. Sun
    # Arguments
        pool_size: int
            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.
        num_rois: number of regions of interest to be used
    # Input shape
        list of two 4D tensors [X_img,X_roi] with shape:
        X_img:
        `(1, channels, rows, cols)` if dim_ordering='th'
        or 4D tensor with shape:
        `(1, rows, cols, channels)` if dim_ordering='tf'.
        X_roi:
        `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)
    # Output shape
        3D tensor with shape:
        `(1, num_rois, channels, pool_size, pool_size)`
    '''

    def __init__(self, pool_size, num_rois, **kwargs):

        # self.dim_ordering = K.image_data_format()

        if K.image_data_format() == 'channels_last':
            self.dim_ordering = 'tf'
        else:
            self.dim_ordering = 'th'

        assert self.dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'

        self.pool_size = pool_size
        self.num_rois = num_rois

        super(RoiPoolingConv, self).__init__(**kwargs)

    def build(self, input_shape):
        if self.dim_ordering == 'th':
            self.nb_channels = input_shape[0][1]
        elif self.dim_ordering == 'tf':
            self.nb_channels = input_shape[0][3]

    def compute_output_shape(self, input_shape):
        if self.dim_ordering == 'th':
            return None, self.num_rois, self.nb_channels, self.pool_size, self.pool_size
        else:
            return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels

    def call(self, x, mask=None):

        assert (len(x) == 2)

        img = x[0]
        rois = x[1]
        # img or base_layers: Tensor("activation_39/Relu:0", shape=(?, ?, ?, 1024), dtype=float32)
        # rois or input_rois: Tensor("input_2:0", shape=(?, ?, 4), dtype=float32)
        # num_rois: 64

        input_shape = K.shape(img)

        outputs = []

        for roi_idx in range(self.num_rois):

            x = rois[0, roi_idx, 0]
            y = rois[0, roi_idx, 1]
            w = rois[0, roi_idx, 2]
            h = rois[0, roi_idx, 3]

            row_length = w / float(self.pool_size)
            col_length = h / float(self.pool_size)

            num_pool_regions = self.pool_size

            # NOTE: the RoiPooling implementation differs between theano and tensorflow due to the lack of a resize op
            # in theano. The theano implementation is much less efficient and leads to long compile times

            if self.dim_ordering == 'th':
                for jy in range(num_pool_regions):
                    for ix in range(num_pool_regions):
                        x1 = x + ix * row_length
                        x2 = x1 + row_length
                        y1 = y + jy * col_length
                        y2 = y1 + col_length

                        x1 = K.cast(x1, 'int32')
                        x2 = K.cast(x2, 'int32')
                        y1 = K.cast(y1, 'int32')
                        y2 = K.cast(y2, 'int32')

                        x2 = x1 + K.maximum(1, x2 - x1)
                        y2 = y1 + K.maximum(1, y2 - y1)

                        new_shape = [input_shape[0], input_shape[1],
                                     y2 - y1, x2 - x1]

                        x_crop = img[:, :, y1:y2, x1:x2]
                        xm = K.reshape(x_crop, new_shape)
                        pooled_val = K.max(xm, axis=(2, 3))
                        outputs.append(pooled_val)

            elif self.dim_ordering == 'tf':
                x = K.cast(x, 'int32')
                y = K.cast(y, 'int32')
                w = K.cast(w, 'int32')
                h = K.cast(h, 'int32')

                rs = tf.image.resize_images(img[:, y:y + h, x:x + w, :], (self.pool_size, self.pool_size))
                outputs.append(rs)

        final_output = K.concatenate(outputs, axis=0)
        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))

        if self.dim_ordering == 'th':
            final_output = K.permute_dimensions(final_output, (0, 1, 4, 2, 3))
        else:
            final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))

        return final_output

    def get_config(self):
        config = {'pool_size': self.pool_size,
                  'num_rois': self.num_rois}
        base_config = super(RoiPoolingConv, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))


'''ResNet50 model for Keras.
# Reference:
- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
Adapted from code contributed by BigMoyan.
'''


class ResNet:

    # def __init__(self):

    @staticmethod
    def get_weight_path(K):
        # if K.image_dim_ordering() == 'th':
        if image_dim_ordering(K) == 'th':
            return 'resnet50_weights_th_dim_ordering_th_kernels_notop.h5'
        else:
            return 'resnet50_weights_tf_dim_ordering_tf_kernels.h5'

    @staticmethod
    def get_img_output_length(width, height):
        def get_output_length(input_length):
            # zero_pad
            input_length += 6
            # apply 4 strided convolutions
            filter_sizes = [7, 3, 1, 1]
            stride = 2
            for filter_size in filter_sizes:
                input_length = (input_length - filter_size + stride) // stride
            return input_length

        return get_output_length(width), get_output_length(height)

    @staticmethod
    def identity_block(input_tensor, kernel_size, filters, stage, block, trainable=True):

        nb_filter1, nb_filter2, nb_filter3 = filters

        # if K.image_dim_ordering() == 'tf':
        if image_dim_ordering(K) == 'tf':
            bn_axis = 3
        else:
            bn_axis = 1

        conv_name_base = 'res' + str(stage) + block + '_branch'
        bn_name_base = 'bn' + str(stage) + block + '_branch'

        x = Convolution2D(nb_filter1, (1, 1), name=conv_name_base + '2a', trainable=trainable)(input_tensor)
        x = FixedBatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)
        x = Activation('relu')(x)

        x = Convolution2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=conv_name_base + '2b',
                          trainable=trainable)(x)
        x = FixedBatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)
        x = Activation('relu')(x)

        x = Convolution2D(nb_filter3, (1, 1), name=conv_name_base + '2c', trainable=trainable)(x)
        x = FixedBatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)

        x = Add()([x, input_tensor])
        x = Activation('relu')(x)
        return x

    @staticmethod
    def identity_block_td(input_tensor, kernel_size, filters, stage, block, trainable=True):

        # identity block time distributed

        nb_filter1, nb_filter2, nb_filter3 = filters
        if image_dim_ordering(K) == 'tf':
            bn_axis = 3
        else:
            bn_axis = 1

        conv_name_base = 'res' + str(stage) + block + '_branch'
        bn_name_base = 'bn' + str(stage) + block + '_branch'

        x = TimeDistributed(Convolution2D(nb_filter1, (1, 1), trainable=trainable, kernel_initializer='normal'),
                            name=conv_name_base + '2a')(input_tensor)
        x = TimeDistributed(FixedBatchNormalization(axis=bn_axis), name=bn_name_base + '2a')(x)
        x = Activation('relu')(x)

        x = TimeDistributed(
            Convolution2D(nb_filter2, (kernel_size, kernel_size), trainable=trainable, kernel_initializer='normal',
                          padding='same'), name=conv_name_base + '2b')(x)
        x = TimeDistributed(FixedBatchNormalization(axis=bn_axis), name=bn_name_base + '2b')(x)
        x = Activation('relu')(x)

        x = TimeDistributed(Convolution2D(nb_filter3, (1, 1), trainable=trainable, kernel_initializer='normal'),
                            name=conv_name_base + '2c')(x)
        x = TimeDistributed(FixedBatchNormalization(axis=bn_axis), name=bn_name_base + '2c')(x)

        x = Add()([x, input_tensor])
        x = Activation('relu')(x)

        return x

    @staticmethod
    def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2), trainable=True):

        nb_filter1, nb_filter2, nb_filter3 = filters
        if image_dim_ordering(K) == 'tf':
            bn_axis = 3
        else:
            bn_axis = 1

        conv_name_base = 'res' + str(stage) + block + '_branch'
        bn_name_base = 'bn' + str(stage) + block + '_branch'

        x = Convolution2D(nb_filter1, (1, 1), strides=strides, name=conv_name_base + '2a', trainable=trainable)(
            input_tensor)
        x = FixedBatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)
        # x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a' )(x)

        x = Activation('relu')(x)

        x = Convolution2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=conv_name_base + '2b',
                          trainable=trainable)(x)
        x = FixedBatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)
        # x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)

        x = Activation('relu')(x)

        x = Convolution2D(nb_filter3, (1, 1), name=conv_name_base + '2c', trainable=trainable)(x)
        x = FixedBatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)
        # x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)

        shortcut = Convolution2D(nb_filter3, (1, 1), strides=strides, name=conv_name_base + '1', trainable=trainable)(
            input_tensor)
        shortcut = FixedBatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)
        # shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1' )(shortcut)

        x = Add()([x, shortcut])
        x = Activation('relu')(x)
        return x

    @staticmethod
    def conv_block_td(input_tensor, kernel_size, filters, stage, block, input_shape, strides=(2, 2),
                      trainable=True):

        # conv block time distributed

        nb_filter1, nb_filter2, nb_filter3 = filters
        if image_dim_ordering(K) == 'tf':
            bn_axis = 3
        else:
            bn_axis = 1

        conv_name_base = 'res' + str(stage) + block + '_branch'
        bn_name_base = 'bn' + str(stage) + block + '_branch'

        x = TimeDistributed(
            Convolution2D(nb_filter1, (1, 1), strides=strides, trainable=trainable, kernel_initializer='normal'),
            input_shape=input_shape, name=conv_name_base + '2a')(input_tensor)
        x = TimeDistributed(FixedBatchNormalization(axis=bn_axis), name=bn_name_base + '2a')(x)
        x = Activation('relu')(x)

        x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), padding='same', trainable=trainable,
                                          kernel_initializer='normal'), name=conv_name_base + '2b')(x)
        x = TimeDistributed(FixedBatchNormalization(axis=bn_axis), name=bn_name_base + '2b')(x)
        x = Activation('relu')(x)

        x = TimeDistributed(Convolution2D(nb_filter3, (1, 1), kernel_initializer='normal'), name=conv_name_base + '2c',
                            trainable=trainable)(x)
        x = TimeDistributed(FixedBatchNormalization(axis=bn_axis), name=bn_name_base + '2c')(x)

        shortcut = TimeDistributed(
            Convolution2D(nb_filter3, (1, 1), strides=strides, trainable=trainable, kernel_initializer='normal'),
            name=conv_name_base + '1')(input_tensor)
        shortcut = TimeDistributed(FixedBatchNormalization(axis=bn_axis), name=bn_name_base + '1')(shortcut)

        x = Add()([x, shortcut])
        x = Activation('relu')(x)
        return x

    @staticmethod
    def nn_base(input_tensor=None, trainable=False):

        # Determine proper input shape
        if image_dim_ordering(K) == 'th':
            input_shape = (3, None, None)
        else:
            input_shape = (None, None, 3)

        if input_tensor is None:
            img_input = Input(shape=input_shape)
        else:
            if not is_keras_tensor(input_tensor):
                img_input = Input(tensor=input_tensor, shape=input_shape)
            else:
                img_input = input_tensor

        if image_dim_ordering(K) == 'tf':
            bn_axis = 3
        else:
            bn_axis = 1

        x = ZeroPadding2D((3, 3))(img_input)

        x = Convolution2D(64, (7, 7), strides=(2, 2), name='conv1', trainable=trainable)(x)
        x = FixedBatchNormalization(axis=bn_axis, name='bn_conv1')(x)
        # x = BatchNormalization(name='bn_conv1', axis=bn_axis)(x)
        x = Activation('relu')(x)
        x = MaxPooling2D((3, 3), strides=(2, 2))(x)

        x = ResNet.conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), trainable=trainable)
        x = ResNet.identity_block(x, 3, [64, 64, 256], stage=2, block='b', trainable=trainable)
        x = ResNet.identity_block(x, 3, [64, 64, 256], stage=2, block='c', trainable=trainable)

        x = ResNet.conv_block(x, 3, [128, 128, 512], stage=3, block='a', trainable=trainable)
        x = ResNet.identity_block(x, 3, [128, 128, 512], stage=3, block='b', trainable=trainable)
        x = ResNet.identity_block(x, 3, [128, 128, 512], stage=3, block='c', trainable=trainable)
        x = ResNet.identity_block(x, 3, [128, 128, 512], stage=3, block='d', trainable=trainable)

        x = ResNet.conv_block(x, 3, [256, 256, 1024], stage=4, block='a', trainable=trainable)
        x = ResNet.identity_block(x, 3, [256, 256, 1024], stage=4, block='b', trainable=trainable)
        x = ResNet.identity_block(x, 3, [256, 256, 1024], stage=4, block='c', trainable=trainable)
        x = ResNet.identity_block(x, 3, [256, 256, 1024], stage=4, block='d', trainable=trainable)
        x = ResNet.identity_block(x, 3, [256, 256, 1024], stage=4, block='e', trainable=trainable)
        x = ResNet.identity_block(x, 3, [256, 256, 1024], stage=4, block='f', trainable=trainable)

        return x

    @staticmethod
    def rpn(base_layers, num_anchors):

        x = Convolution2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal',
                          name='rpn_conv1')(base_layers)

        x_class = Convolution2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform',
                                name='rpn_out_class')(x)
        x_regr = Convolution2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero',
                               name='rpn_out_regress')(x)

        return [x_class, x_regr, base_layers]

    @staticmethod
    def classifier(base_layers, input_rois, num_rois, nb_classes=21, trainable=False):

        # compile times on theano tend to be very high, so we use smaller ROI pooling regions to workaround

        if K.backend() == 'tensorflow':
            pooling_regions = 14
            input_shape = (num_rois, 14, 14, 1024)
        elif K.backend() == 'theano':
            pooling_regions = 7
            input_shape = (num_rois, 1024, 7, 7)

        out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])
        out = ResNet.classifier_layers(out_roi_pool, input_shape=input_shape, trainable=True)

        out = TimeDistributed(Flatten())(out)

        out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'),
                                    name='dense_class_{}'.format(nb_classes))(out)
        # note: no regression target for bg class
        out_regr = TimeDistributed(Dense(4 * (nb_classes - 1), activation='linear', kernel_initializer='zero'),
                                   name='dense_regress_{}'.format(nb_classes))(out)
        return [out_class, out_regr]

    @staticmethod
    def classifier_layers(x, input_shape, trainable=False):

        # compile times on theano tend to be very high, so we use smaller ROI pooling regions to workaround
        # (hence a smaller stride in the region that follows the ROI pool)
        if K.backend() == 'tensorflow':
            x = ResNet.conv_block_td(x, 3, [512, 512, 2048], stage=5, block='a', input_shape=input_shape,
                                     strides=(2, 2),
                                     trainable=trainable)
        elif K.backend() == 'theano':
            x = ResNet.conv_block_td(x, 3, [512, 512, 2048], stage=5, block='a', input_shape=input_shape,
                                     strides=(1, 1),
                                     trainable=trainable)

        x = ResNet.identity_block_td(x, 3, [512, 512, 2048], stage=5, block='b', trainable=trainable)
        x = ResNet.identity_block_td(x, 3, [512, 512, 2048], stage=5, block='c', trainable=trainable)
        x = TimeDistributed(AveragePooling2D((7, 7)), name='avg_pool')(x)

        return x


'''Simple ResNet50 model for Keras.
# Reference:
- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
Adapted from code contributed by BigMoyan.
'''


class SimpleResNet:

    # def __init__(self):

    @staticmethod
    def get_weight_path(K):
        # if K.image_dim_ordering() == 'th':
        if image_dim_ordering(K) == 'th':
            return 'resnet50_weights_th_dim_ordering_th_kernels_notop.h5'
        else:
            return 'resnet50_weights_tf_dim_ordering_tf_kernels.h5'

    @staticmethod
    def get_img_output_length(width, height):
        def get_output_length(input_length):
            # zero_pad
            input_length += 6
            # apply 4 strided convolutions
            filter_sizes = [7, 3, 1, 1]
            stride = 2
            for filter_size in filter_sizes:
                input_length = (input_length - filter_size + stride) // stride
            return input_length

        return get_output_length(width), get_output_length(height)

    @staticmethod
    def identity_block(input_tensor, kernel_size, filters, stage, block, trainable=True):

        nb_filter1, nb_filter2, nb_filter3 = filters

        # if K.image_dim_ordering() == 'tf':
        if image_dim_ordering(K) == 'tf':
            bn_axis = 3
        else:
            bn_axis = 1

        conv_name_base = 'res' + str(stage) + block + '_branch'
        bn_name_base = 'bn' + str(stage) + block + '_branch'

        x = Convolution2D(nb_filter1, (1, 1), name=conv_name_base + '2a', trainable=trainable)(input_tensor)
        # x = FixedBatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)
        x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)
        x = Activation('relu')(x)

        x = Convolution2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=conv_name_base + '2b',
                          trainable=trainable)(x)
        # x = FixedBatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)
        x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)
        x = Activation('relu')(x)

        x = Convolution2D(nb_filter3, (1, 1), name=conv_name_base + '2c', trainable=trainable)(x)
        # x = FixedBatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)
        x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)

        x = Add()([x, input_tensor])
        x = Activation('relu')(x)
        return x

    # @staticmethod
    # def identity_block_td(input_tensor, kernel_size, filters, stage, block, trainable=True):
    #
    #     # identity block time distributed
    #
    #     nb_filter1, nb_filter2, nb_filter3 = filters
    #     if image_dim_ordering(K) == 'tf':
    #         bn_axis = 3
    #     else:
    #         bn_axis = 1
    #
    #     conv_name_base = 'res' + str(stage) + block + '_branch'
    #     bn_name_base = 'bn' + str(stage) + block + '_branch'
    #
    #     x = TimeDistributed(Convolution2D(nb_filter1, (1, 1), trainable=trainable, kernel_initializer='normal'),
    #                         name=conv_name_base + '2a')(input_tensor)
    #     x = TimeDistributed(FixedBatchNormalization(axis=bn_axis), name=bn_name_base + '2a')(x)
    #     x = Activation('relu')(x)
    #
    #     x = TimeDistributed(
    #         Convolution2D(nb_filter2, (kernel_size, kernel_size), trainable=trainable, kernel_initializer='normal',
    #                       padding='same'), name=conv_name_base + '2b')(x)
    #     x = TimeDistributed(FixedBatchNormalization(axis=bn_axis), name=bn_name_base + '2b')(x)
    #     x = Activation('relu')(x)
    #
    #     x = TimeDistributed(Convolution2D(nb_filter3, (1, 1), trainable=trainable, kernel_initializer='normal'),
    #                         name=conv_name_base + '2c')(x)
    #     x = TimeDistributed(FixedBatchNormalization(axis=bn_axis), name=bn_name_base + '2c')(x)
    #
    #     x = Add()([x, input_tensor])
    #     x = Activation('relu')(x)
    #
    #     return x

    @staticmethod
    def conv_block(input_tensor, kernel_size, filters, stage, block, input_shape=None, strides=(2, 2), trainable=True):

        nb_filter1, nb_filter2, nb_filter3 = filters
        if image_dim_ordering(K) == 'tf':
            bn_axis = 3
        else:
            bn_axis = 1

        conv_name_base = 'res' + str(stage) + block + '_branch'
        bn_name_base = 'bn' + str(stage) + block + '_branch'

        if input_shape == None:
            x = Convolution2D(nb_filter1, (1, 1), strides=strides, name=conv_name_base + '2a', trainable=trainable)(
                input_tensor)
        else:
            print('tensor-shape', input_tensor)
            x = Convolution2D(nb_filter1, (1, 1), strides=strides, input_shape=input_shape, name=conv_name_base + '2a',
                              trainable=trainable)(
                input_tensor)

        # x = Convolution2D(nb_filter1, (1, 1), strides=strides, name=conv_name_base + '2a', trainable=trainable)(
        #     input_tensor)

        # x = FixedBatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)
        x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2a')(x)

        x = Activation('relu')(x)

        x = Convolution2D(nb_filter2, (kernel_size, kernel_size), padding='same', name=conv_name_base + '2b',
                          trainable=trainable)(x)
        # x = FixedBatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)
        x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')(x)

        x = Activation('relu')(x)

        x = Convolution2D(nb_filter3, (1, 1), name=conv_name_base + '2c', trainable=trainable)(x)
        # x = FixedBatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)
        x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)

        shortcut = Convolution2D(nb_filter3, (1, 1), strides=strides, name=conv_name_base + '1', trainable=trainable)(
            input_tensor)
        # shortcut = FixedBatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)
        shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)

        x = Add()([x, shortcut])
        x = Activation('relu')(x)
        return x

    # @staticmethod
    # def conv_block_td(input_tensor, kernel_size, filters, stage, block, input_shape, strides=(2, 2),
    #                   trainable=True):
    #
    #     # conv block time distributed
    #
    #     nb_filter1, nb_filter2, nb_filter3 = filters
    #     if image_dim_ordering(K) == 'tf':
    #         bn_axis = 3
    #     else:
    #         bn_axis = 1
    #
    #     conv_name_base = 'res' + str(stage) + block + '_branch'
    #     bn_name_base = 'bn' + str(stage) + block + '_branch'
    #
    #     x = TimeDistributed(
    #         Convolution2D(nb_filter1, (1, 1), strides=strides, trainable=trainable, kernel_initializer='normal'),
    #         input_shape=input_shape, name=conv_name_base + '2a')(input_tensor)
    #     x = TimeDistributed(FixedBatchNormalization(axis=bn_axis), name=bn_name_base + '2a')(x)
    #     x = Activation('relu')(x)
    #
    #     x = TimeDistributed(Convolution2D(nb_filter2, (kernel_size, kernel_size), padding='same', trainable=trainable,
    #                                       kernel_initializer='normal'), name=conv_name_base + '2b')(x)
    #     x = TimeDistributed(FixedBatchNormalization(axis=bn_axis), name=bn_name_base + '2b')(x)
    #     x = Activation('relu')(x)
    #
    #     x = TimeDistributed(Convolution2D(nb_filter3, (1, 1), kernel_initializer='normal'), name=conv_name_base + '2c',
    #                         trainable=trainable)(x)
    #     x = TimeDistributed(FixedBatchNormalization(axis=bn_axis), name=bn_name_base + '2c')(x)
    #
    #     shortcut = TimeDistributed(
    #         Convolution2D(nb_filter3, (1, 1), strides=strides, trainable=trainable, kernel_initializer='normal'),
    #         name=conv_name_base + '1')(input_tensor)
    #     shortcut = TimeDistributed(FixedBatchNormalization(axis=bn_axis), name=bn_name_base + '1')(shortcut)
    #
    #     x = Add()([x, shortcut])
    #     x = Activation('relu')(x)
    #     return x

    @staticmethod
    def nn_base(input_tensor=None, trainable=False):

        # Determine proper input shape
        if image_dim_ordering(K) == 'th':
            input_shape = (3, None, None)
        else:
            input_shape = (None, None, 3)

        if input_tensor is None:
            img_input = Input(shape=input_shape)
        else:
            if not is_keras_tensor(input_tensor):
                img_input = Input(tensor=input_tensor, shape=input_shape)
            else:
                img_input = input_tensor

        if image_dim_ordering(K) == 'tf':
            bn_axis = 3
        else:
            bn_axis = 1

        x = ZeroPadding2D((3, 3))(img_input)

        x = Convolution2D(64, (7, 7), strides=(2, 2), name='conv1', trainable=trainable)(x)
        # x = FixedBatchNormalization(axis=bn_axis, name='bn_conv1')(x)
        x = BatchNormalization(name='bn_conv1', axis=bn_axis)(x)
        x = Activation('relu')(x)
        x = MaxPooling2D((3, 3), strides=(2, 2))(x)

        x = SimpleResNet.conv_block(x, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), trainable=trainable)
        x = SimpleResNet.identity_block(x, 3, [64, 64, 256], stage=2, block='b', trainable=trainable)
        x = SimpleResNet.identity_block(x, 3, [64, 64, 256], stage=2, block='c', trainable=trainable)

        x = SimpleResNet.conv_block(x, 3, [128, 128, 512], stage=3, block='a', trainable=trainable)
        x = SimpleResNet.identity_block(x, 3, [128, 128, 512], stage=3, block='b', trainable=trainable)
        x = SimpleResNet.identity_block(x, 3, [128, 128, 512], stage=3, block='c', trainable=trainable)
        x = SimpleResNet.identity_block(x, 3, [128, 128, 512], stage=3, block='d', trainable=trainable)

        x = SimpleResNet.conv_block(x, 3, [256, 256, 1024], stage=4, block='a', trainable=trainable)
        x = SimpleResNet.identity_block(x, 3, [256, 256, 1024], stage=4, block='b', trainable=trainable)
        x = SimpleResNet.identity_block(x, 3, [256, 256, 1024], stage=4, block='c', trainable=trainable)
        x = SimpleResNet.identity_block(x, 3, [256, 256, 1024], stage=4, block='d', trainable=trainable)
        x = SimpleResNet.identity_block(x, 3, [256, 256, 1024], stage=4, block='e', trainable=trainable)
        x = SimpleResNet.identity_block(x, 3, [256, 256, 1024], stage=4, block='f', trainable=trainable)

        return x

    @staticmethod
    def rpn(base_layers, num_anchors):
        ## base_layers: Tensor("activation_39/Relu:0", shape=(?, ?, ?, 1024), dtype=float32)
        ## num_anchors: 9

        x = Convolution2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal',
                          name='rpn_conv1')(base_layers)
        ## x : Tensor("rpn_conv1/Relu:0", shape=(?, ?, ?, 512), dtype=float32)

        x_class = Convolution2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform',
                                name='rpn_out_class')(x)
        ## x_class: Tensor("rpn_out_class/Sigmoid:0", shape=(?, ?, ?, 9), dtype=float32)

        x_regr = Convolution2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero',
                               name='rpn_out_regress')(x)
        ## x_regr: Tensor("rpn_out_regress/BiasAdd:0", shape=(?, ?, ?, 36), dtype=float32)

        return [x_class, x_regr, base_layers]

    @staticmethod
    def classifier(base_layers, input_rois, num_rois, nb_classes=21, trainable=False):
        # base_layers: Tensor("activation_39/Relu:0", shape=(?, ?, ?, 1024), dtype=float32)
        # input_rois: Tensor("input_2:0", shape=(?, ?, 4), dtype=float32)
        # num_rois: 64

        # compile times on theano tend to be very high, so we use smaller ROI pooling regions to workaround

        if K.backend() == 'tensorflow':
            pooling_regions = 14
            # input_shape = (num_rois, 14, 14, 1024)
            input_shape = (1, num_rois, 14, 14, 1024)
        elif K.backend() == 'theano':
            pooling_regions = 7
            input_shape = (num_rois, 1024, 7, 7)

        # base_layers: Tensor("activation_39/Relu:0", shape=(?, ?, ?, 1024), dtype=float32)
        # input_rois: Tensor("input_2:0", shape=(?, ?, 4), dtype=float32)
        # num_rois: 64
        out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])
        # out_roi_pool: Tensor("roi_pooling_conv/transpose:0", shape=(1, 64, 14, 14, 1024), dtype=float32)

        out = SimpleResNet.classifier_layers(out_roi_pool, input_shape=input_shape, trainable=True)
        ## out: Tensor("avg_pool/Reshape_1:0", shape=(1, 64, 1, 1, 2048), dtype=float32)

        out = TimeDistributed(Flatten())(out)
        ## out: Tensor("time_distributed/Reshape_2:0", shape=(1, 64, 2048), dtype=float32)

        # out = Flatten()(out)
        ## out: Tensor("flatten/Reshape:0", shape=(1, 131072), dtype=float32)
        ## due to non TimeDistributed(Flatten())
        ## ValueError: `TimeDistributed` Layer should be passed an `input_shape ` with at least 3 dimensions, received: [1, 131072]

        ## ValueError: Index out of range using input dim 2; input has only 2 dims for 'loss_1/dense_class_21_loss/strided_slice' (op: 'StridedSlice') with input shapes: [?,?], [3], [3], [3] and with computed input tensors: input[3] = <1 1 1>.

        out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'),
                                    name='dense_class_{}'.format(nb_classes))(out)
        ## out_class: Tensor("time_distributed/Reshape_2:0", shape=(1, 64, 2048), dtype=float32)

        # out_class = Dense(nb_classes, activation='softmax', kernel_initializer='zero',
        #                             name='dense_class_{}'.format(nb_classes))(out)
        ## out_class: Tensor("dense_class_21/Softmax:0", shape=(1, 21), dtype=float32)

        # note: no regression target for bg class
        # out_regr = TimeDistributed(Dense(4 * (nb_classes - 1), activation='linear', kernel_initializer='zero'),
        #                            name='dense_regress_{}'.format(nb_classes))(out)
        out_regr = Dense(4 * (nb_classes - 1), activation='linear', kernel_initializer='zero',
                         name='dense_regress_{}'.format(nb_classes))(out)
        ## Tensor("dense_regress_21/BiasAdd:0", shape=(1, 64, 80), dtype=float32)

        return [out_class, out_regr]

    @staticmethod
    def classifier_layers(x, input_shape, trainable=False):
        # x or out_roi_pool: Tensor("roi_pooling_conv/transpose:0", shape=(1, 64, 14, 14, 1024), dtype=float32)

        # compile times on theano tend to be very high, so we use smaller ROI pooling regions to workaround
        # (hence a smaller stride in the region that follows the ROI pool)
        if K.backend() == 'tensorflow':
            ## x = ResNet.conv_block(x, 3, [512, 512, 2048], stage=5, block='a', input_shape=input_shape,
            ##                         strides=(2, 2),
            ##                         trainable=trainable)
            ## For non TD call, we get following error.
            ## ValueError: Input 0 of layer res5a_branch2a is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [1, 64, 14, 14, 1024]

            x = ResNet.conv_block_td(x, 3, [512, 512, 2048], stage=5, block='a', input_shape=input_shape,
                                     strides=(2, 2),
                                     trainable=trainable)
            # x: Tensor("activation_42/Relu:0", shape=(1, 64, 7, 7, 2048), dtype=float32)


        elif K.backend() == 'theano':
            x = ResNet.conv_block_td(x, 3, [512, 512, 2048], stage=5, block='a', input_shape=input_shape,
                                     strides=(1, 1),
                                     trainable=trainable)

        ## x = SimpleResNet.identity_block(x, 3, [512, 512, 2048], stage=5, block='b', trainable=trainable)
        ## ValueError: Input 0 of layer res5b_branch2a is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [1, 64, 7, 7, 2048]

        x = ResNet.identity_block_td(x, 3, [512, 512, 2048], stage=5, block='b', trainable=trainable)
        # x: Tensor("activation_45/Relu:0", shape=(1, 64, 7, 7, 2048), dtype=float32)

        x = ResNet.identity_block_td(x, 3, [512, 512, 2048], stage=5, block='c', trainable=trainable)
        # x: Tensor("activation_48/Relu:0", shape=(1, 64, 7, 7, 2048), dtype=float32)

        ## x = AveragePooling2D((7, 7), name='avg_pool')(x)
        ## ValueError: Input 0 of layer avg_pool is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: [1, 64, 7, 7, 2048]

        x = TimeDistributed(AveragePooling2D((7, 7)), name='avg_pool')(x)

        return x


class simple_parser:

    @staticmethod
    def get_data(input_path):
        found_bg = False
        all_imgs = {}

        classes_count = {}

        class_mapping = {}

        visualise = True

        with open(input_path, 'r') as f:

            print('Parsing annotation files')

            for line in f:
                line_split = line.strip().split(',')
                (filename, x1, y1, x2, y2, class_name, videoname, frameid) = line_split
                if filename == 'filename':
                    continue

                if class_name not in classes_count:
                    classes_count[class_name] = 1
                else:
                    classes_count[class_name] += 1

                if class_name not in class_mapping:
                    if class_name == 'bg' and found_bg == False:
                        print(
                            'Found class name with special name bg. Will be treated as a background region (this is usually for hard negative mining).')
                        found_bg = True
                    class_mapping[class_name] = len(class_mapping)

                if filename not in all_imgs:
                    all_imgs[filename] = {}

                    img = cv2.imread(filename)
                    (rows, cols) = img.shape[:2]
                    all_imgs[filename]['filepath'] = filename
                    all_imgs[filename]['width'] = cols
                    all_imgs[filename]['height'] = rows
                    all_imgs[filename]['bboxes'] = []
                    if np.random.randint(0, 6) > 0:
                        all_imgs[filename]['imageset'] = 'trainval'
                    else:
                        all_imgs[filename]['imageset'] = 'test'

                all_imgs[filename]['bboxes'].append(
                    {'class': class_name, 'x1': int(x1), 'x2': int(x2), 'y1': int(y1), 'y2': int(y2)})

            all_data = []
            for key in all_imgs:
                all_data.append(all_imgs[key])

            # make sure the bg class is last in the list
            if found_bg:
                if class_mapping['bg'] != len(class_mapping) - 1:
                    key_to_switch = \
                        [key for key in class_mapping.keys() if class_mapping[key] == len(class_mapping) - 1][0]
                    val_to_switch = class_mapping['bg']
                    class_mapping['bg'] = len(class_mapping) - 1
                    class_mapping[key_to_switch] = val_to_switch

            return all_data, classes_count, class_mapping


"""VGG16 model for Keras.
# Reference
- [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)
"""


class vgg16:

    @staticmethod
    def get_weight_path(K):
        # if K.image_dim_ordering() == 'th':
        if image_dim_ordering(K) == 'th':
            print('pretrained weights not available for VGG with theano backend')
            return
        else:
            return 'vgg16_weights_tf_dim_ordering_tf_kernels.h5'

    @staticmethod
    def get_img_output_length(width, height):
        def get_output_length(input_length):
            return input_length // 16

        return get_output_length(width), get_output_length(height)

    @staticmethod
    def nn_base(input_tensor=None, trainable=False):

        # Determine proper input shape
        # if K.image_dim_ordering() == 'th':
        if image_dim_ordering(K) == 'th':
            input_shape = (3, None, None)
        else:
            input_shape = (None, None, 3)

        if input_tensor is None:
            img_input = Input(shape=input_shape)
        else:
            # if not K.is_keras_tensor(input_tensor):
            if not is_keras_tensor(input_tensor):
                img_input = Input(tensor=input_tensor, shape=input_shape)
            else:
                img_input = input_tensor

        # if K.image_dim_ordering() == 'tf':
        if image_dim_ordering(K) == 'tf':
            bn_axis = 3
        else:
            bn_axis = 1

        # Block 1
        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)
        x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)
        x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)

        # Block 2
        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)
        x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)
        x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)

        # Block 3
        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)
        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)
        x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)
        x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)

        # Block 4
        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)
        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)
        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)
        x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)

        # Block 5
        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)
        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)
        x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)
        # x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)

        return x

    @staticmethod
    def rpn(base_layers, num_anchors):

        x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(
            base_layers)

        x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(
            x)
        x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero',
                        name='rpn_out_regress')(x)

        return [x_class, x_regr, base_layers]

    @staticmethod
    def classifier(base_layers, input_rois, num_rois, nb_classes=21, trainable=False):

        # compile times on theano tend to be very high, so we use smaller ROI pooling regions to workaround

        if K.backend() == 'tensorflow':
            pooling_regions = 7
            input_shape = (num_rois, 7, 7, 512)
        elif K.backend() == 'theano':
            pooling_regions = 7
            input_shape = (num_rois, 512, 7, 7)

        out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])

        out = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)
        out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out)
        out = TimeDistributed(Dropout(0.5))(out)
        out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)
        out = TimeDistributed(Dropout(0.5))(out)

        out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'),
                                    name='dense_class_{}'.format(nb_classes))(out)
        # note: no regression target for bg class
        out_regr = TimeDistributed(Dense(4 * (nb_classes - 1), activation='linear', kernel_initializer='zero'),
                                   name='dense_regress_{}'.format(nb_classes))(out)

        return [out_class, out_regr]


class cvatParser:

    def __init__(self):
        print("cvatParser initialized.")

    @staticmethod
    def cvat_video_to_simple_csv():
        annot_path = './data/annotations'
        videos_path = './data/videos'
        images_path = './data/images'

        annots = [os.path.join(annot_path, s) for s in os.listdir(annot_path)]
        idx = 0
        df = pd.DataFrame(columns=['filename', 'x1', 'y1', 'x2', 'y2', 'class_name', 'video_name', 'frame'])

        for annot in annots:
            print(' ')
            print('Processing annotation file:', annot)
            # if annot != './data/annotations/0814 080540.xml':
            #     continue

            try:
                idx += 1

                et = ET.parse(annot)
                root = et.getroot()
                element_source = root.find('.//source')
                video_name = element_source.text
                video_name_simple = video_name.replace(' ', '-')
                stop_frame = root.find('.//stop_frame')
                element_tracks = root.findall('.//track')

                for track in element_tracks:
                    warned = False
                    class_name = track.attrib['label']
                    track_id = track.attrib['id']

                    print('label:', class_name)
                    boxes = track.findall('box')
                    print('frames:')
                    key_x2 = 0
                    key_y2 = 0
                    for box in boxes:

                        box_att = box.attrib
                        frame = box_att['frame']
                        is_keyframe = box_att['keyframe']
                        x1 = box_att['xtl']
                        y1 = box_att['ytl']
                        x2 = box_att['xbr']
                        y2 = box_att['ybr']
                        outside = box_att['outside']

                        if float(x2) - float(x1) < 64 and float(y2) - float(y1) < 64:
                            print('X', frame, end='')
                            continue

                        if is_keyframe == '1':
                            key_x2 = x2
                            key_y2 = y2
                            warned = False
                        else:
                            if (key_x2 == x2 and key_y2 == y2 and is_keyframe == '0'):
                                # to avoid lost tracked annotations. e.g. forgot to annotate during interpolation.
                                if not warned:
                                    print(' ')
                                    print('WARN: label:', class_name, ', track_id:', track_id,
                                          ', maybe lose tracking from keyframe: ', frame)
                                    warned = True

                                continue

                        print(' ', frame, end='')
                        image_name = cvatParser.makeImageName(frame, images_path, video_name_simple)
                        df = df.append({'filename': image_name,
                                        'x1': round(float(x1)),
                                        'y1': round(float(y1)),
                                        'x2': round(float(x2)),
                                        'y2': round(float(y2)),
                                        'class_name': class_name,
                                        'video_name': video_name,
                                        'frame': frame}, ignore_index=True)

                    print(' ')  # end of frames

            except Exception as e:
                print(' ')
                print('ERROR:', e)
                continue

            df.to_csv('./training.csv', index=False)

            # every annotation xml file for its own video.
            print('Dump images:')
            video_frame = 0
            video = cv2.VideoCapture(os.path.join(videos_path, video_name))
            vdf = df[df['video_name'] == video_name]
            success, image = video.read()
            while (success):

                if sum(vdf['frame'] == str(video_frame)) > 0:
                    image_name = cvatParser.makeImageName(str(video_frame), images_path, video_name_simple)
                    cv2.imwrite(image_name, image)
                    print(' ', image_name, end='')

                success, image = video.read()
                video_frame = video_frame + 1

            print(' ')  # Dump images:

        print('done parsing')

    @staticmethod
    def makeImageName(frame, images_path, video_name_simple):
        return os.path.join(images_path, video_name_simple + "-F" + frame + ".png")


class FasterRCNNModel:

    def __init__(self):
        # self.number_anchors = 9
        print("instance initialized.")

    # start Training here.
    def train(self, parser):

        sys.setrecursionlimit(40000)

        (options, args) = parser.parse_args()

        if not options.train_path:  # if filename is not given
            parser.error('Error: path to training data must be specified. Pass --path to command line')

        if options.parser == 'pascal_voc':
            # from keras_frcnn.pascal_voc_parser import get_data
            parser = pascal_voc_parser()

        elif options.parser == 'simple':
            # from keras_frcnn.simple_parser import get_data
            parser = simple_parser()
        else:
            raise ValueError("Command line option parser must be one of 'pascal_voc' or 'simple'")

        # pass the settings from the command line, and persist them in the config object
        C = Config()

        C.use_horizontal_flips = bool(options.horizontal_flips)
        C.use_vertical_flips = bool(options.vertical_flips)
        C.rot_90 = bool(options.rot_90)

        C.model_path = options.output_weight_path
        C.num_rois = int(options.num_rois)

        if options.network == 'vgg':
            C.network = 'vgg'
            # from keras_frcnn import vgg as nn
            nn = vgg16
        elif options.network == 'resnet50':
            # from keras_frcnn import resnet as nn
            nn = ResNet
            C.network = 'resnet50'
        elif options.network == 'simple_resnet50':
            # from keras_frcnn import resnet as nn
            nn = SimpleResNet
            C.network = 'simple_resnet50'
        else:
            print('Not a valid model')
            raise ValueError

        # check if weight path was passed via command line
        if options.input_weight_path:
            C.base_net_weights = options.input_weight_path
        else:
            # set the path to weights based on backend and model
            C.base_net_weights = nn.get_weight_path()

        all_imgs, classes_count, class_mapping = parser.get_data(options.train_path)

        if 'bg' not in classes_count:
            classes_count['bg'] = 0
            class_mapping['bg'] = len(class_mapping)

        C.class_mapping = class_mapping

        inv_map = {v: k for k, v in class_mapping.items()}

        print('Training images per class:')
        pprint.pprint(classes_count)
        print('Num classes (including bg) = {}'.format(len(classes_count)))

        config_output_filename = options.config_filename

        with open(config_output_filename, 'wb') as config_f:
            pickle.dump(C, config_f)
            print('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(
                config_output_filename))

        random.shuffle(all_imgs)

        num_imgs = len(all_imgs)

        train_imgs = [s for s in all_imgs if s['imageset'] == 'trainval']
        val_imgs = [s for s in all_imgs if s['imageset'] == 'test']

        print('Num train samples {}'.format(len(train_imgs)))
        print('Num val samples {}'.format(len(val_imgs)))

        data_gen_train = data_generators.get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length,
                                                       image_dim_ordering(K), mode='train')
        data_gen_val = data_generators.get_anchor_gt(val_imgs, classes_count, C, nn.get_img_output_length,
                                                     image_dim_ordering(K), mode='val')

        if image_dim_ordering(K) == 'th':
            input_shape_img = (3, None, None)
        else:
            input_shape_img = (None, None, 3)

        img_input = Input(shape=input_shape_img)
        roi_input = Input(shape=(None, 4))

        # define the base network (resnet here, can be VGG, Inception, etc)
        shared_layers = nn.nn_base(img_input, trainable=True)

        # define the RPN, built on the base layers
        num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)
        rpn = nn.rpn(shared_layers, num_anchors)

        classifier = nn.classifier(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count), trainable=True)

        model_rpn = Model(img_input, rpn[:2])
        model_classifier = Model([img_input, roi_input], classifier)

        # this is a model that holds both the RPN and the classifier, used to load/save weights for the models
        model_all = Model([img_input, roi_input], rpn[:2] + classifier)

        try:
            print('loading weights from {}'.format(C.base_net_weights))
            model_rpn.load_weights(C.base_net_weights, by_name=True)
            model_classifier.load_weights(C.base_net_weights, by_name=True)
        except:
            print('Could not load pretrained model weights. Weights can be found in the keras application folder \
        		https://github.com/fchollet/keras/tree/master/keras/applications')

        optimizer = Adam(lr=1e-5)
        optimizer_classifier = Adam(lr=1e-5)

        model_rpn.compile(optimizer=optimizer,
                          loss=[lossers.rpn_loss_cls(num_anchors), lossers.rpn_loss_regr(num_anchors)])
        model_classifier.compile(optimizer=optimizer_classifier,
                                 loss=[lossers.class_loss_cls, lossers.class_loss_regr(len(classes_count) - 1)],
                                 metrics={'dense_class_{}'.format(len(classes_count)): 'accuracy'})
        model_all.compile(optimizer='sgd', loss='mae')

        epoch_length = 1000
        num_epochs = int(options.num_epochs)
        iter_num = 0

        losses = np.zeros((epoch_length, 5))
        rpn_accuracy_rpn_monitor = []
        rpn_accuracy_for_epoch = []
        start_time = time.time()

        best_loss = np.Inf

        class_mapping_inv = {v: k for k, v in class_mapping.items()}
        print('Starting training')

        vis = True
        print("model_rpn:")
        model_rpn.summary()
        for epoch_num in range(num_epochs):

            progbar = generic_utils.Progbar(epoch_length)
            print('Epoch {}/{}'.format(epoch_num + 1, num_epochs))

            while True:
                try:

                    if len(rpn_accuracy_rpn_monitor) == epoch_length and C.verbose:
                        mean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor)) / len(rpn_accuracy_rpn_monitor)
                        rpn_accuracy_rpn_monitor = []
                        print(
                            'Average number of overlapping bounding boxes from RPN = {} for {} previous iterations'.format(
                                mean_overlapping_bboxes, epoch_length))
                        if mean_overlapping_bboxes == 0:
                            print(
                                'RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')

                    X, Y, img_data = next(data_gen_train)
                    # print(X.shape, Y[0].shape)

                    loss_rpn = model_rpn.train_on_batch(X, Y)

                    P_rpn = model_rpn.predict_on_batch(X)

                    roi_helpers = roi_helper()
                    R = roi_helpers.rpn_to_roi(P_rpn[0], P_rpn[1], C, image_dim_ordering(K), use_regr=True,
                                               overlap_thresh=0.7, max_boxes=300)
                    # note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format
                    X2, Y1, Y2, IouS = roi_helpers.calc_iou(R, img_data, C, class_mapping)

                    if X2 is None:
                        rpn_accuracy_rpn_monitor.append(0)
                        rpn_accuracy_for_epoch.append(0)
                        continue

                    neg_samples = np.where(Y1[0, :, -1] == 1)
                    pos_samples = np.where(Y1[0, :, -1] == 0)

                    if len(neg_samples) > 0:
                        neg_samples = neg_samples[0]
                    else:
                        neg_samples = []

                    if len(pos_samples) > 0:
                        pos_samples = pos_samples[0]
                    else:
                        pos_samples = []

                    rpn_accuracy_rpn_monitor.append(len(pos_samples))
                    rpn_accuracy_for_epoch.append((len(pos_samples)))

                    if C.num_rois > 1:
                        if len(pos_samples) < C.num_rois // 2:
                            selected_pos_samples = pos_samples.tolist()
                        else:
                            selected_pos_samples = np.random.choice(pos_samples, C.num_rois // 2,
                                                                    replace=False).tolist()
                        try:
                            selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples),
                                                                    replace=False).tolist()
                        except:
                            selected_neg_samples = np.random.choice(neg_samples, C.num_rois - len(selected_pos_samples),
                                                                    replace=True).tolist()

                        sel_samples = selected_pos_samples + selected_neg_samples
                    else:
                        # in the extreme case where num_rois = 1, we pick a random pos or neg sample
                        selected_pos_samples = pos_samples.tolist()
                        selected_neg_samples = neg_samples.tolist()
                        if np.random.randint(0, 2):
                            sel_samples = random.choice(neg_samples)
                        else:
                            sel_samples = random.choice(pos_samples)

                    loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]],
                                                                 [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])

                    losses[iter_num, 0] = loss_rpn[1]
                    losses[iter_num, 1] = loss_rpn[2]

                    losses[iter_num, 2] = loss_class[1]
                    losses[iter_num, 3] = loss_class[2]
                    losses[iter_num, 4] = loss_class[3]

                    progbar.update(iter_num + 1, [('rpn_cls', losses[iter_num, 0]), ('rpn_regr', losses[iter_num, 1]),
                                                  ('detector_cls', losses[iter_num, 2]),
                                                  ('detector_regr', losses[iter_num, 3])])

                    iter_num += 1

                    if iter_num == epoch_length:
                        loss_rpn_cls = np.mean(losses[:, 0])
                        loss_rpn_regr = np.mean(losses[:, 1])
                        loss_class_cls = np.mean(losses[:, 2])
                        loss_class_regr = np.mean(losses[:, 3])
                        class_acc = np.mean(losses[:, 4])

                        mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)
                        rpn_accuracy_for_epoch = []

                        if C.verbose:
                            print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {}'.format(
                                mean_overlapping_bboxes))
                            print('Classifier accuracy for bounding boxes from RPN: {}'.format(class_acc))
                            print('Loss RPN classifier: {}'.format(loss_rpn_cls))
                            print('Loss RPN regression: {}'.format(loss_rpn_regr))
                            print('Loss Detector classifier: {}'.format(loss_class_cls))
                            print('Loss Detector regression: {}'.format(loss_class_regr))
                            print('Elapsed time: {}'.format(time.time() - start_time))

                        curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr
                        iter_num = 0
                        start_time = time.time()

                        if curr_loss < best_loss:
                            if C.verbose:
                                print('Total loss decreased from {} to {}, saving weights'.format(best_loss, curr_loss))
                            best_loss = curr_loss
                            model_all.save_weights(C.model_path)

                        break

                except Exception as e:
                    print('Exception: {}'.format(e))
                    continue

        print('Training complete, exiting.')

    def test_video(self, parser):

        bbox_threshold = 0.66
        visualise = True

        sys.setrecursionlimit(40000)
        os.environ["TF_FORCE_GPU_ALLOW_GROWTH"] = "true"
        os.environ["CUDA_VISIBLE_DEVICES"] = "1"  # for GPU 1.

        (options, args) = parser.parse_args()

        if not options.train_path:  # if filename is not given
            parser.error('Error: path to test data must be specified. Pass --path to command line')

        config_output_filename = options.config_filename

        with open(config_output_filename, 'rb') as f_in:
            C = pickle.load(f_in)

        if C.network == 'resnet50':
            # import keras_frcnn.resnet as nn
            nn = ResNet
        elif C.network == 'vgg':
            nn = vgg16
        elif C.network == 'simple_resnet50':
            nn = SimpleResNet
            # import keras_frcnn.vgg as nn

        # turn off any data augmentation at test time
        C.use_horizontal_flips = False
        C.use_vertical_flips = False
        C.rot_90 = False

        img_path = options.train_path

        class_mapping, class_to_color, format_img, get_real_coordinates, model_classifier_only, model_rpn = self.testing_init(
            C, nn, options)

        all_imgs = []

        classes = {}

        img_name = img_path
        if not img_name.lower().endswith(('.avi', '.mp4')):
            print(img_name, " is not video. Testing exists.")
            exit(0)

            # continue

        print(img_name)
        st = time.time()
        # filepath = os.path.join(img_path, img_name)

        # img = cv2.imread(filepath)
        vs = cv2.VideoCapture(img_name)
        ok, img = vs.read()
        fps = vs.get(cv2.CAP_PROP_FPS)
        video_width = int(vs.get(cv2.CAP_PROP_FRAME_WIDTH))
        video_height = int(vs.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fourcc = cv2.VideoWriter_fourcc(*'H264')

        outfile = img_name + '-detected.mp4'
        video_out = cv2.VideoWriter(outfile, fourcc, fps, (video_width, video_height), True)
        idx = 0
        while ok:
            st = time.time()
            all_dets = self.testing_draw(C, bbox_threshold, class_mapping, class_to_color, format_img,
                                         get_real_coordinates, img, model_classifier_only, model_rpn)

            if len(all_dets) > 0:
                print('Elapsed time = {}'.format(time.time() - st))
                print(all_dets)
                # cv2.imshow('img', img)
                # cv2.waitKey(0)
            else:
                print("Checked ", img_name, ' ', idx, ' lapsed: {}'.format(time.time() - st))

            # cv2.imwrite('./results_imgs/{}.png'.format(idx),img)
            # sudo apt-get install ffmpeg x264 libx264-dev
            video_out.write(img)
            ok, img = vs.read()
            idx += 1

        print('Video detected completed. Please check, ', outfile)
        video_out.release()
        vs.release()

    def test(self, parser):

        bbox_threshold = 0.6
        visualise = True

        sys.setrecursionlimit(40000)
        os.environ["TF_FORCE_GPU_ALLOW_GROWTH"] = "true"
        os.environ["CUDA_VISIBLE_DEVICES"] = "1"  # for GPU 1.

        (options, args) = parser.parse_args()

        if not options.train_path:  # if filename is not given
            parser.error('Error: path to test data must be specified. Pass --path to command line')

        config_output_filename = options.config_filename

        with open(config_output_filename, 'rb') as f_in:
            C = pickle.load(f_in)

        if C.network == 'resnet50':
            # import keras_frcnn.resnet as nn
            nn = ResNet
        elif C.network == 'vgg':
            nn = vgg16
        elif C.network == 'simple_resnet50':
            nn = SimpleResNet
            # import keras_frcnn.vgg as nn

        # turn off any data augmentation at test time
        C.use_horizontal_flips = False
        C.use_vertical_flips = False
        C.rot_90 = False

        img_path = options.train_path

        class_mapping, class_to_color, format_img, get_real_coordinates, model_classifier_only, model_rpn = self.testing_init(
            C, nn, options)

        all_imgs = []

        classes = {}

        for idx, img_name in enumerate(os.listdir(img_path)):  # sorted()
            if not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):
                continue
            print(img_name)
            st = time.time()
            filepath = os.path.join(img_path, img_name)

            img = cv2.imread(filepath)

            all_dets = self.testing_draw(C, bbox_threshold, class_mapping, class_to_color, format_img,
                                         get_real_coordinates, img, model_classifier_only, model_rpn)

            if len(all_dets) > 0:
                print('Elapsed time = {}'.format(time.time() - st))
                print(all_dets)
                cv2.imshow('img', img)
                cv2.waitKey(0)
            else:
                print("Checked ", img_name, ' lapsed: {}'.format(time.time() - st))
        # cv2.imwrite('./results_imgs/{}.png'.format(idx),img)

    def testing_draw(self, C, bbox_threshold, class_mapping, class_to_color, format_img, get_real_coordinates, img,
                     model_classifier_only, model_rpn):
        X, ratio = format_img(img, C)
        if image_dim_ordering(K) == 'tf':
            X = np.transpose(X, (0, 2, 3, 1))
        # get the feature maps and output from the RPN
        [Y1, Y2, F] = model_rpn.predict(X)
        R = roi_helper.rpn_to_roi(Y1, Y2, C, image_dim_ordering(K), overlap_thresh=0.7)
        # convert from (x1,y1,x2,y2) to (x,y,w,h)
        R[:, 2] -= R[:, 0]
        R[:, 3] -= R[:, 1]
        # apply the spatial pyramid pooling to the proposed regions
        bboxes = {}
        probs = {}
        for jk in range(R.shape[0] // C.num_rois + 1):
            ROIs = np.expand_dims(R[C.num_rois * jk:C.num_rois * (jk + 1), :], axis=0)
            if ROIs.shape[1] == 0:
                break

            if jk == R.shape[0] // C.num_rois:
                # pad R
                curr_shape = ROIs.shape
                target_shape = (curr_shape[0], C.num_rois, curr_shape[2])
                ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)
                ROIs_padded[:, :curr_shape[1], :] = ROIs
                ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]
                ROIs = ROIs_padded

            [P_cls, P_regr] = model_classifier_only.predict([F, ROIs])

            for ii in range(P_cls.shape[1]):

                if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):
                    continue

                cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]

                if cls_name not in bboxes:
                    bboxes[cls_name] = []
                    probs[cls_name] = []

                (x, y, w, h) = ROIs[0, ii, :]

                cls_num = np.argmax(P_cls[0, ii, :])
                try:
                    (tx, ty, tw, th) = P_regr[0, ii, 4 * cls_num:4 * (cls_num + 1)]
                    tx /= C.classifier_regr_std[0]
                    ty /= C.classifier_regr_std[1]
                    tw /= C.classifier_regr_std[2]
                    th /= C.classifier_regr_std[3]
                    x, y, w, h = roi_helper.apply_regr(x, y, w, h, tx, ty, tw, th)
                except:
                    pass
                bboxes[cls_name].append(
                    [C.rpn_stride * x, C.rpn_stride * y, C.rpn_stride * (x + w), C.rpn_stride * (y + h)])
                probs[cls_name].append(np.max(P_cls[0, ii, :]))
        all_dets = []
        for key in bboxes:
            bbox = np.array(bboxes[key])

            new_boxes, new_probs = roi_helper.non_max_suppression_fast(bbox, np.array(probs[key]),
                                                                       overlap_thresh=0.5)
            for jk in range(new_boxes.shape[0]):
                (x1, y1, x2, y2) = new_boxes[jk, :]

                (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)

                cv2.rectangle(img, (real_x1, real_y1), (real_x2, real_y2), (
                    int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])), 2)

                textLabel = '{}: {}'.format(key, int(100 * new_probs[jk]))
                all_dets.append((key, 100 * new_probs[jk]))

                (retval, baseLine) = cv2.getTextSize(textLabel, cv2.FONT_HERSHEY_COMPLEX, 1, 1)
                textOrg = (real_x1, real_y1 - 0)

                cv2.rectangle(img, (textOrg[0] - 5, textOrg[1] + baseLine - 5),
                              (textOrg[0] + retval[0] + 5, textOrg[1] - retval[1] - 5), (0, 0, 0), 2)
                cv2.rectangle(img, (textOrg[0] - 5, textOrg[1] + baseLine - 5),
                              (textOrg[0] + retval[0] + 5, textOrg[1] - retval[1] - 5), (255, 255, 255), -1)
                cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)
        return all_dets

    def testing_init(self, C, nn, options):
        def format_img_size(img, C):
            """ formats the image size based on config """
            img_min_side = float(C.im_size)
            (height, width, _) = img.shape

            if width <= height:
                ratio = img_min_side / width
                new_height = int(ratio * height)
                new_width = int(img_min_side)
            else:
                ratio = img_min_side / height
                new_width = int(ratio * width)
                new_height = int(img_min_side)
            img = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)
            return img, ratio

        def format_img_channels(img, C):
            """ formats the image channels based on config """
            img = img[:, :, (2, 1, 0)]
            img = img.astype(np.float32)
            img[:, :, 0] -= C.img_channel_mean[0]
            img[:, :, 1] -= C.img_channel_mean[1]
            img[:, :, 2] -= C.img_channel_mean[2]
            img /= C.img_scaling_factor
            img = np.transpose(img, (2, 0, 1))
            img = np.expand_dims(img, axis=0)
            return img

        def format_img(img, C):
            """ formats an image for model prediction based on config """
            img, ratio = format_img_size(img, C)
            img = format_img_channels(img, C)
            return img, ratio

        # Method to transform the coordinates of the bounding box to its original size
        def get_real_coordinates(ratio, x1, y1, x2, y2):

            real_x1 = int(round(x1 // ratio))
            real_y1 = int(round(y1 // ratio))
            real_x2 = int(round(x2 // ratio))
            real_y2 = int(round(y2 // ratio))

            return (real_x1, real_y1, real_x2, real_y2)

        class_mapping = C.class_mapping
        if 'bg' not in class_mapping:
            class_mapping['bg'] = len(class_mapping)
        class_mapping = {v: k for k, v in class_mapping.items()}
        print(class_mapping)
        class_to_color = {class_mapping[v]: np.random.randint(0, 255, 3) for v in class_mapping}
        C.num_rois = int(options.num_rois)
        if C.network == 'resnet50' or C.network == 'simple_resnet50':
            num_features = 1024
        elif C.network == 'vgg':
            num_features = 512
        if image_dim_ordering(K) == 'th':
            input_shape_img = (3, None, None)
            input_shape_features = (num_features, None, None)
        else:
            input_shape_img = (None, None, 3)
            input_shape_features = (None, None, num_features)
        img_input = Input(shape=input_shape_img)
        roi_input = Input(shape=(C.num_rois, 4))
        feature_map_input = Input(shape=input_shape_features)
        # define the base network (resnet here, can be VGG, Inception, etc)
        shared_layers = nn.nn_base(img_input, trainable=True)
        # define the RPN, built on the base layers
        num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)
        rpn_layers = nn.rpn(shared_layers, num_anchors)
        classifier = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping),
                                   trainable=True)
        model_rpn = Model(img_input, rpn_layers)
        model_classifier_only = Model([feature_map_input, roi_input], classifier)
        model_classifier = Model([feature_map_input, roi_input], classifier)
        print('Loading weights from {}'.format(C.model_path))
        model_rpn.load_weights(C.model_path, by_name=True)
        model_classifier.load_weights(C.model_path, by_name=True)
        model_rpn.compile(optimizer='sgd', loss='mse')
        model_classifier.compile(optimizer='sgd', loss='mse')
        return class_mapping, class_to_color, format_img, get_real_coordinates, model_classifier_only, model_rpn


def main():
    # os.environ["CUDA_VISIBLE_DEVICES"] = "1"  # for GPU 1.

    parser = OptionParser()
    parser.add_option("-m", "--mode", dest="mode", help="train or test or to_simple_parser")

    # options for testing
    # parser.add_option("-p", "--path", dest="test_path", help="Path to test data.")
    # parser.add_option("-n", "--num_rois", type="int", dest="num_rois",
    # help="Number of ROIs per iteration. Higher means more memory use.", default=32)
    # parser.add_option("--config_filename", dest="config_filename", help=
    # "Location to read the metadata related to the training (generated when training).",
    #                   default="config.pickle")
    # parser.add_option("--network", dest="network", help="Base network to use. Supports vgg or resnet50.",
    #                   default='resnet50')

    # option for training
    parser.add_option("-p", "--path", dest="train_path", help="Path to training data.")
    parser.add_option("-o", "--parser", dest="parser", help="Parser to use. One of simple or pascal_voc",
                      default="pascal_voc")
    parser.add_option("-n", "--num_rois", type="int", dest="num_rois", help="Number of RoIs to process at once.",
                      default=64)
    parser.add_option("--network", dest="network", help="Base network to use. Supports vgg or resnet50.",
                      default='resnet50')
    parser.add_option("--hf", dest="horizontal_flips",
                      help="Augment with horizontal flips in training. (Default=false).", action="store_true",
                      default=False)
    parser.add_option("--vf", dest="vertical_flips",
                      help="Augment with vertical flips in training. (Default=false).", action="store_true",
                      default=False)
    parser.add_option("--rot", "--rot_90", dest="rot_90",
                      help="Augment with 90 degree rotations in training. (Default=false).",
                      action="store_true", default=False)
    parser.add_option("--num_epochs", type="int", dest="num_epochs", help="Number of epochs.", default=2000)
    parser.add_option("--config_filename", dest="config_filename", help=
    "Location to store all the metadata related to the training (to be used when testing).",
                      default="config.pickle")
    parser.add_option("--output_weight_path", dest="output_weight_path", help="Output path for weights.",
                      default='./model_frcnn.hdf5')
    parser.add_option("--input_weight_path", dest="input_weight_path",
                      help="Input path for weights. If not specified, will try to load default weights provided by keras.",
                      default='./model_frcnn.hdf5')

    (options, args) = parser.parse_args()

    m = FasterRCNNModel()
    if options.mode == 'train':
        m.train(parser)

    if options.mode == 'test':
        m.test(parser)

    if options.mode == 'test-video':
        m.test_video(parser)

    if options.mode == 'to_simple_parser':
        cvatParser.cvat_video_to_simple_csv()


if __name__ == '__main__':
    main()
